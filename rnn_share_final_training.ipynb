{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import music21\n",
    "import music21.instrument\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('generated/melodyData2.txt', 'r') as f:\n",
    "    parts = eval(f.read())\n",
    "    print (\"Number of parts:\" + str(len(parts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parts to train on\n",
    "train_all_parts = True\n",
    "num_data = int(1e9) if train_all_parts else 250\n",
    "# max length of each part\n",
    "max_sequence_length = int(1e9) if train_all_parts else 300 \n",
    "\n",
    "# one-hot encoding\n",
    "encodings = {}\n",
    "encodingIndex = 0\n",
    "for part in parts[:num_data]:\n",
    "    for note in part[:max_sequence_length]:\n",
    "        if note not in encodings:\n",
    "            encodings[note] = encodingIndex\n",
    "            encodingIndex += 1\n",
    "\n",
    "# decoder constructed by reversing one-hot encoding\n",
    "decodings = {}\n",
    "for k, v in encodings.items():\n",
    "    decodings[v] = k\n",
    "\n",
    "# encode everything in a\n",
    "data_encoded = []\n",
    "for part in parts[:num_data]:\n",
    "    data_encoded.append([encodings[note] for note in part[:max_sequence_length]])\n",
    "\n",
    "num_data = min(num_data, len(data_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the data\n",
    "def generate_data(data_encoded, encodings):\n",
    "    X = []\n",
    "    Y = []\n",
    "    # given data_encoded, generate training data by looping \n",
    "    for i in range(len(data_encoded)):\n",
    "        currentX = []\n",
    "        currentY = []\n",
    "        for j in range(len(data_encoded[i])-1):\n",
    "            currentX.append(data_encoded[i][j])\n",
    "            currentY.append(data_encoded[i][j+1])\n",
    "        X.append(currentX)\n",
    "        Y.append(currentY)\n",
    "\n",
    "    X_onehot = []\n",
    "    for seq in X:\n",
    "        onehot = np.zeros((len(seq), len(encodings)))\n",
    "        for note_index in range(len(seq)):\n",
    "            onehot[note_index][seq[note_index]] = 1\n",
    "        X_onehot.append(onehot)\n",
    "    X = X_onehot\n",
    "    \n",
    "    Y_onehot = []\n",
    "    for seq in Y:\n",
    "        onehot = np.zeros((len(seq), len(encodings)))\n",
    "        for note_index in range(len(seq)):\n",
    "            onehot[note_index][seq[note_index]] = 1\n",
    "            \n",
    "        Y_onehot.append(onehot)\n",
    "    Y = Y_onehot\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "X_train, Y_train = generate_data(data_encoded[:math.floor(num_data*0.7)], encodings)\n",
    "#X_test, Y_test = generate_data(data_encoded[10+math.floor(num_data*0.7)], encodings)\n",
    "X_test, Y_test = generate_data(data_encoded[math.floor(num_data*0.7):], encodings)\n",
    "\n",
    "# pads sequences so we can convert to numpy arrays\n",
    "X_train = tf.keras.utils.pad_sequences(X_train, padding='pre')\n",
    "Y_train = tf.keras.utils.pad_sequences(Y_train, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Masking(mask_value=0, batch_input_shape=(1, None, len(encodings))),\n",
    "    tf.keras.layers.LSTM(64, stateful=True, return_sequences=True),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(encodings), activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "timeTaken = time.time()\n",
    "\n",
    "# Train the model one time step at a time\n",
    "accuracy = 0\n",
    "epochs = 2\n",
    "for epoch in range(epochs):\n",
    "    print(f'Epoch {epoch + 1}/{epochs}')\n",
    "    total_loss = 0\n",
    "    for i, sequence in enumerate(X_train):\n",
    "        # Reset states at the beginning of each sequence\n",
    "        model.reset_states() \n",
    "        x = sequence.reshape((1, sequence.shape[0], len(encodings)))\n",
    "        y = Y_train[i].reshape((1, sequence.shape[0], len(encodings)))\n",
    "        loss, note_accuracy = model.train_on_batch(x, y)\n",
    "        total_loss += loss\n",
    "    accuracy += note_accuracy\n",
    "    print(f\"Accuracy {accuracy/(epoch+1)}, Loss {total_loss/len(X_train)}\")\n",
    "    print(f\"Epoch time {time.time() - timeTaken}\")\n",
    "    timeTaken = time.time()\n",
    "# 6.27 seconds for 5 epochs\n",
    "# 1000 seconds for 1 epoch (full data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "model.save_weights(f\"./checkpoints/model_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_parts = 30\n",
    "# tests model accuracy\n",
    "\n",
    "correct = 0.0\n",
    "total = 0\n",
    "\n",
    "\n",
    "for i in range(min(test_parts, len(X_test))):\n",
    "    # reset model\n",
    "    model.reset_states()\n",
    "    print (i)\n",
    "    \n",
    "    for j in range(len(X_test[i])):\n",
    "        # use predict_on_batch\n",
    "        \n",
    "        curr_note = X_test[i][j].reshape(1, 1, X_test[i][j].shape[0])\n",
    "        pred = model.predict_on_batch(curr_note)\n",
    "        \n",
    "        if (np.argmax(pred) == np.argmax(Y_test[i][j])):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    # replace this, use train_on\n",
    "print(correct/total)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
