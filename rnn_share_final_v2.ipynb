{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-11 19:19:35.592361: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "import os \n",
    "import music21\n",
    "import music21.instrument\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import math\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parts:6410\n"
     ]
    }
   ],
   "source": [
    "with open('generated/melodyData.txt', 'r') as f:\n",
    "    parts = eval(f.read())\n",
    "    print (\"Number of parts:\" + str(len(parts)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# number of parts to train on\n",
    "train_all_parts = True\n",
    "num_data = int(1e9) if train_all_parts else 250\n",
    "# max length of each part\n",
    "max_sequence_length = int(1e9) if train_all_parts else 300 \n",
    "\n",
    "# one-hot encoding\n",
    "encodings = {}\n",
    "encodingIndex = 0\n",
    "for part in parts[:num_data]:\n",
    "    for note in part[:max_sequence_length]:\n",
    "        if note not in encodings:\n",
    "            encodings[note] = encodingIndex\n",
    "            encodingIndex += 1\n",
    "\n",
    "# decoder constructed by reversing one-hot encoding\n",
    "decodings = {}\n",
    "for k, v in encodings.items():\n",
    "    decodings[v] = k\n",
    "\n",
    "# encode everything in a\n",
    "data_encoded = []\n",
    "for part in parts[:num_data]:\n",
    "    data_encoded.append([encodings[note] for note in part[:max_sequence_length]])\n",
    "\n",
    "num_data = min(num_data, len(data_encoded))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# one-hot encode the data\n",
    "def generate_data(data_encoded, encodings):\n",
    "    X = []\n",
    "    Y = []\n",
    "    # given data_encoded, generate training data by looping \n",
    "    for i in range(len(data_encoded)):\n",
    "        currentX = []\n",
    "        currentY = []\n",
    "        for j in range(len(data_encoded[i])-1):\n",
    "            currentX.append(data_encoded[i][j])\n",
    "            currentY.append(data_encoded[i][j+1])\n",
    "        X.append(currentX)\n",
    "        Y.append(currentY)\n",
    "\n",
    "    X_onehot = []\n",
    "    for seq in X:\n",
    "        onehot = np.zeros((len(seq), len(encodings)))\n",
    "        for note_index in range(len(seq)):\n",
    "            onehot[note_index][seq[note_index]] = 1\n",
    "        X_onehot.append(onehot)\n",
    "    X = X_onehot\n",
    "    \n",
    "    Y_onehot = []\n",
    "    for seq in Y:\n",
    "        onehot = np.zeros((len(seq), len(encodings)))\n",
    "        for note_index in range(len(seq)):\n",
    "            onehot[note_index][seq[note_index]] = 1\n",
    "            \n",
    "        Y_onehot.append(onehot)\n",
    "    Y = Y_onehot\n",
    "\n",
    "    return X, Y\n",
    "\n",
    "X_train, Y_train = generate_data(data_encoded[:math.floor(num_data*0.7)], encodings)\n",
    "#X_test, Y_test = generate_data(data_encoded[10+math.floor(num_data*0.7)], encodings)\n",
    "X_test, Y_test = generate_data(data_encoded[math.floor(num_data*0.7):], encodings)\n",
    "\n",
    "# pads sequences so we can convert to numpy arrays\n",
    "X_train = tf.keras.utils.pad_sequences(X_train, padding='pre')\n",
    "Y_train = tf.keras.utils.pad_sequences(Y_train, padding='pre')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " masking (Masking)           (1, None, 188)            0         \n",
      "                                                                 \n",
      " lstm (LSTM)                 (1, None, 64)             64768     \n",
      "                                                                 \n",
      " dense (Dense)               (1, None, 64)             4160      \n",
      "                                                                 \n",
      " dense_1 (Dense)             (1, None, 64)             4160      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (1, None, 188)            12220     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 85308 (333.23 KB)\n",
      "Trainable params: 85308 (333.23 KB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.layers.Masking(mask_value=0, batch_input_shape=(1, None, len(encodings))),\n",
    "    tf.keras.layers.LSTM(64, stateful=True, return_sequences=True),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(64, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(encodings), activation='softmax')\n",
    "])\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "\n",
    "model.compile(optimizer=optimizer, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "if True:\n",
    "    model.load_weights('checkpoints/model_full_v1.h5')\n",
    "else:\n",
    "\n",
    "    timeTaken = time.time()\n",
    "\n",
    "    # Train the model one time step at a time\n",
    "    accuracy = 0\n",
    "    epochs = 2\n",
    "    for epoch in range(epochs):\n",
    "        print(f'Epoch {epoch + 1}/{epochs}')\n",
    "        total_loss = 0\n",
    "        for i, sequence in enumerate(X_train):\n",
    "            # Reset states at the beginning of each sequence\n",
    "            model.reset_states() \n",
    "            x = sequence.reshape((1, sequence.shape[0], len(encodings)))\n",
    "            y = Y_train[i].reshape((1, sequence.shape[0], len(encodings)))\n",
    "            loss, note_accuracy = model.train_on_batch(x, y)\n",
    "            total_loss += loss\n",
    "        accuracy += note_accuracy\n",
    "        print(f\"Accuracy {accuracy/(epoch+1)}, Loss {total_loss/len(X_train)}\")\n",
    "        print(f\"Epoch time {time.time() - timeTaken}\")\n",
    "        timeTaken = time.time()\n",
    "# 6.27 seconds for 5 epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[6], line 17\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(X_test[i])):\n\u001b[1;32m     14\u001b[0m     \u001b[38;5;66;03m# use predict_on_batch\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     curr_note \u001b[38;5;241m=\u001b[39m X_test[i][j]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m, X_test[i][j]\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m])\n\u001b[0;32m---> 17\u001b[0m     pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict_on_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcurr_note\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (np\u001b[38;5;241m.\u001b[39margmax(pred) \u001b[38;5;241m==\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(Y_test[i][j])):\n\u001b[1;32m     20\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/keras/src/engine/training.py:2876\u001b[0m, in \u001b[0;36mModel.predict_on_batch\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m   2874\u001b[0m _disallow_inside_tf_function(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpredict_on_batch\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   2875\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdistribute_strategy\u001b[38;5;241m.\u001b[39mscope():\n\u001b[0;32m-> 2876\u001b[0m     iterator \u001b[38;5;241m=\u001b[39m \u001b[43mdata_adapter\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msingle_batch_iterator\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2877\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdistribute_strategy\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\n\u001b[1;32m   2878\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2879\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmake_predict_function()\n\u001b[1;32m   2880\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpredict_function(iterator)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/keras/src/engine/data_adapter.py:1941\u001b[0m, in \u001b[0;36msingle_batch_iterator\u001b[0;34m(strategy, x, y, sample_weight, class_weight)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     data \u001b[38;5;241m=\u001b[39m (x, y, sample_weight)\n\u001b[1;32m   1940\u001b[0m _check_data_cardinality(data)\n\u001b[0;32m-> 1941\u001b[0m dataset \u001b[38;5;241m=\u001b[39m \u001b[43mtf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1942\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m class_weight:\n\u001b[1;32m   1943\u001b[0m     dataset \u001b[38;5;241m=\u001b[39m dataset\u001b[38;5;241m.\u001b[39mmap(_make_class_weight_map_fn(class_weight))\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/tensorflow/python/data/ops/dataset_ops.py:740\u001b[0m, in \u001b[0;36mDatasetV2.from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m    736\u001b[0m \u001b[38;5;66;03m# Loaded lazily due to a circular dependency (dataset_ops ->\u001b[39;00m\n\u001b[1;32m    737\u001b[0m \u001b[38;5;66;03m# from_tensors_op -> dataset_ops).\u001b[39;00m\n\u001b[1;32m    738\u001b[0m \u001b[38;5;66;03m# pylint: disable=g-import-not-at-top,protected-access\u001b[39;00m\n\u001b[1;32m    739\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpython\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdata\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mops\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m from_tensors_op\n\u001b[0;32m--> 740\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfrom_tensors_op\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_from_tensors\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/tensorflow/python/data/ops/from_tensors_op.py:23\u001b[0m, in \u001b[0;36m_from_tensors\u001b[0;34m(tensors, name)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_from_tensors\u001b[39m(tensors, name):  \u001b[38;5;66;03m# pylint: disable=unused-private-name\u001b[39;00m\n\u001b[0;32m---> 23\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_TensorDataset\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/tensorflow/python/data/ops/from_tensors_op.py:35\u001b[0m, in \u001b[0;36m_TensorDataset.__init__\u001b[0;34m(self, element, name)\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tensors \u001b[38;5;241m=\u001b[39m structure\u001b[38;5;241m.\u001b[39mto_tensor_list(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_structure, element)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_name \u001b[38;5;241m=\u001b[39m name\n\u001b[0;32m---> 35\u001b[0m variant_tensor \u001b[38;5;241m=\u001b[39m \u001b[43mgen_dataset_ops\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtensor_dataset\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     36\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_tensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     37\u001b[0m \u001b[43m    \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstructure\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_flat_tensor_shapes\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_structure\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     38\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_metadata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mSerializeToString\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;28msuper\u001b[39m()\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m(variant_tensor)\n",
      "File \u001b[0;32m~/.pyenv/versions/3.11.6/lib/python3.11/site-packages/tensorflow/python/ops/gen_dataset_ops.py:7629\u001b[0m, in \u001b[0;36mtensor_dataset\u001b[0;34m(components, output_shapes, metadata, name)\u001b[0m\n\u001b[1;32m   7627\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tld\u001b[38;5;241m.\u001b[39mis_eager:\n\u001b[1;32m   7628\u001b[0m   \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 7629\u001b[0m     _result \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_FastPathExecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   7630\u001b[0m \u001b[43m      \u001b[49m\u001b[43m_ctx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mTensorDataset\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcomponents\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43moutput_shapes\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m   7631\u001b[0m \u001b[43m      \u001b[49m\u001b[43moutput_shapes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mmetadata\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   7632\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _result\n\u001b[1;32m   7633\u001b[0m   \u001b[38;5;28;01mexcept\u001b[39;00m _core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "test_parts = 30\n",
    "# tests model accuracy\n",
    "\n",
    "correct = 0.0\n",
    "total = 0\n",
    "\n",
    "\n",
    "for i in range(min(test_parts, len(X_test))):\n",
    "    # reset model\n",
    "    model.reset_states()\n",
    "    print (i)\n",
    "    \n",
    "    for j in range(len(X_test[i])):\n",
    "        # use predict_on_batch\n",
    "        \n",
    "        curr_note = X_test[i][j].reshape(1, 1, X_test[i][j].shape[0])\n",
    "        pred = model.predict_on_batch(curr_note)\n",
    "        \n",
    "        if (np.argmax(pred) == np.argmax(Y_test[i][j])):\n",
    "            correct += 1\n",
    "        total += 1\n",
    "    # replace this, use train_on\n",
    "print(correct/total)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating Melody 1:\n",
      "------------------------------\n",
      "[(8, '0.25'), (8, '0.25'), (12, '0.25'), (13, '0.25'), (8, '0.25'), (15, '0.25'), (8, '0.25'), (8, '0.25'), (12, '0.75'), (10, '0.25'), (10, '0.25'), (10, '0.25'), (10, '0.25'), (0, '0.25'), (10, '0.25'), (10, '0.25'), (10, '0.25'), (0, '0.25'), (10, '0.25'), (10, '0.25'), (15, '0.25'), (10, '0.25'), (10, '0.25'), (0, '0.25'), (10, '0.25'), (0, '0.25'), (10, '0.25'), (0, '0.25'), (10, '0.25'), (10, '0.25'), (14, '0.25'), (0, '0.25'), (10, '0.25'), (10, '0.25'), (0, '0.25'), (10, '0.25'), (0, '0.25'), (10, '0.25'), (0, '0.25'), (10, '0.25'), (0, '0.25'), (10, '0.25'), (0, '0.25'), (10, '0.25'), (0, '0.50'), (10, '0.25'), (0, '0.25'), (10, '0.25'), (0, '0.25'), (15, '0.25'), (0, '0.25'), (10, '0.25'), (0, '0.25'), (10, '0.25'), (0, '0.25'), (15, '0.25'), (0, '0.25'), (15, '0.25'), (0, '0.25'), (15, '0.25'), (0, '0.25')]\n",
      "Generating Melody 2:\n",
      "------------------------------\n",
      "[(15, '0.25'), (0, '0.25'), (10, '0.25'), (0, '0.25'), (11, '0.25'), (0, '0.25'), (10, '0.25'), (0, '0.25'), (10, '0.25'), (0, '0.25'), (15, '0.25'), (0, '0.25'), (15, '0.25'), (0, '0.25'), (10, '0.25'), (0, '0.25'), (10, '0.25'), (0, '0.25'), (17, '0.25'), (0, '0.25'), (17, '0.25'), (0, '0.25'), (19, '0.25'), (10, '0.25'), (17, '0.25'), (0, '0.25'), (21, '0.25'), (0, '0.25'), (10, '0.25'), (0, '0.25'), (12, '0.25'), (15, '0.25'), (12, '0.25'), (0, '0.25'), (12, '0.25'), (0, '0.25'), (18, '0.25'), (17, '0.25'), (17, '0.25'), (0, '0.25'), (12, '0.25'), (20, '0.25'), (19, '0.25'), (17, '0.25'), (15, '0.25'), (19, '0.25'), (17, '0.25'), (0, '0.25'), (12, '0.25'), (17, '0.25'), (16, '0.25'), (19, '0.25'), (19, '0.25'), (19, '0.25'), (17, '0.25'), (0, '0.25'), (17, '0.25'), (0, '0.25'), (12, '0.25'), (10, '0.25'), (17, '0.25'), (14, '0.25'), (16, '0.25'), (17, '0.25')]\n",
      "Generating Melody 3:\n",
      "------------------------------\n",
      "[(12, '0.25'), (15, '0.25'), (17, '0.25'), (0, '0.25'), (10, '0.25'), (12, '0.25'), (10, '0.25'), (0, '0.25'), (5, '0.25'), (0, '0.25'), (17, '0.25'), (0, '0.25'), (12, '0.25'), (0, '0.25'), (11, '0.25'), (0, '0.25'), (10, '0.25'), (10, '0.25'), (10, '0.25'), (0, '0.25'), (19, '0.25'), (0, '0.25'), (11, '0.25'), (0, '0.25'), (6, '0.25'), (0, '0.25'), (13, '0.25'), (0, '0.25'), (14, '0.25'), (0, '0.25'), (11, '0.25'), (0, '0.25'), (11, '0.25'), (0, '0.25'), (6, '0.25'), (0, '0.25'), (6, '0.25'), (0, '0.25'), (6, '0.25'), (10, '0.25'), (5, '0.25'), (0, '0.25'), (5, '0.25'), (13, '0.25'), (20, '0.25'), (0, '0.25'), (9, '0.25'), (0, '0.25'), (5, '0.25'), (0, '0.25'), (3, '0.25'), (7, '0.25'), (8, '0.25'), (0, '0.25'), (5, '0.25'), (0, '0.25'), (5, '0.25'), (0, '0.25'), (5, '0.25'), (0, '0.25'), (12, '0.25'), (0, '0.75')]\n"
     ]
    }
   ],
   "source": [
    "# generate \n",
    "\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "def validNote(note):\n",
    "    # ('minor', '4/4')\n",
    "    if note[0] == 'minor' or note[0] == 'major':\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def generate_melodies(num_melodies=3, measures=4, beats_per_measure=4):\n",
    "    global model  # Use the global model variable\n",
    "    beats_per_melody = measures * beats_per_measure\n",
    "    forbidden_first_notes = set()\n",
    "    \n",
    "    original_states = copy.deepcopy(model.get_weights())\n",
    "\n",
    "    generated_melodies = []\n",
    "\n",
    "    melody_num = 0\n",
    "\n",
    "    while melody_num < num_melodies:\n",
    "  \n",
    "        while True:\n",
    "            one_hot = np.zeros(len(encodings))\n",
    "            one_hot[1] = 1  \n",
    "\n",
    "            pred = model.predict_on_batch(np.array([[one_hot]]))[0][0]\n",
    "\n",
    "            if len(pred) != len(encodings):\n",
    "                raise ValueError(f\"Prediction size mismatch: {len(pred)} vs {len(encodings)}\")\n",
    "\n",
    "            first_note_index = np.random.choice(len(pred), p=pred)\n",
    "            first_note = decodings[first_note_index]\n",
    "\n",
    "            if first_note not in forbidden_first_notes and validNote(first_note) and first_note[0] != 'C':\n",
    "                break\n",
    "\n",
    "            model.set_weights(original_states)\n",
    "\n",
    "        generated_notes = [first_note]\n",
    "        previous_note_index = first_note_index\n",
    "\n",
    "        beats = float(first_note[1])\n",
    "\n",
    "        #for _ in range(1, beats_per_melody):\n",
    "        while beats < beats_per_melody:\n",
    "            one_hot = np.zeros(len(encodings))\n",
    "            one_hot[previous_note_index] = 1\n",
    "            \n",
    "            pred = model.predict_on_batch(np.array([[one_hot]]))[0][0]\n",
    "\n",
    "            if len(pred) != len(encodings):\n",
    "                raise ValueError(f\"Prediction size mismatch: {len(pred)} vs {len(encodings)}\")\n",
    "\n",
    "            next_index = np.random.choice(len(pred), p=pred)\n",
    "            generated_note = decodings[next_index]\n",
    "            generated_notes.append(generated_note)\n",
    "            previous_note_index = next_index\n",
    "\n",
    "            if generated_note[0] == 'C':\n",
    "                beats += 1\n",
    "            else:\n",
    "                beats += float(generated_note[1])\n",
    "\n",
    "        \n",
    "        # TODO: if the length of notes is wrong or a single note is invalid, repeat the whole process and don't add it\n",
    "\n",
    "        model.set_weights(original_states)\n",
    "\n",
    "        # add it if it's valid\n",
    "        if beats == beats_per_melody and all(validNote(note) for note in generated_notes):\n",
    "            melody_output = []\n",
    "            for measure in range(measures):\n",
    "                measure_notes = generated_notes[measure * beats_per_melody: (measure + 1) * beats_per_melody]\n",
    "                melody_output.extend(measure_notes)\n",
    "            \n",
    "            generated_melodies.append(melody_output)\n",
    "\n",
    "            # add the first note to the forbidden list\n",
    "            forbidden_first_notes.add(first_note)\n",
    "\n",
    "            # if successful set this\n",
    "            melody_num += 1\n",
    "            print(f\"Generating Melody {melody_num}:\")\n",
    "            print(\"-\" * 30)\n",
    "            print (generated_melodies[-1])\n",
    "\n",
    "    return generated_melodies\n",
    "\n",
    "melodies = generate_melodies(num_melodies=3)\n",
    "#for melody in melodies:\n",
    "#    for measure in melody:\n",
    "#        print(measure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "model.save_weights(f\"./checkpoints/model_{datetime.now().strftime('%Y-%m-%d_%H-%M-%S')}.h5\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
