{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import music21\n",
    "import music21.instrument\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "227 encodings\n"
     ]
    }
   ],
   "source": [
    "# read from melodyData.txt\n",
    "with open('generated/melodyDataShort.txt', 'r') as f:\n",
    "    parts = eval(f.read())\n",
    "\n",
    "# one-hot encoding\n",
    "# encodings is like a dictionary \n",
    "encodings = {}\n",
    "encodingIndex = 0\n",
    "for part in parts:\n",
    "    for note in part:\n",
    "        if note not in encodings:\n",
    "            encodings[note] = encodingIndex\n",
    "            encodingIndex += 1\n",
    "print(len(encodings), \"encodings\")\n",
    "\n",
    "decodings = {}\n",
    "for k, v in encodings.items():\n",
    "    decodings[v] = k        \n",
    "\n",
    "# the actual data that is encoded using the created dictionary\n",
    "# encode everything in indices first\n",
    "data_encoded = []\n",
    "for part in parts:\n",
    "    data_encoded.append([encodings[note] for note in part])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0, 0, 0, 1, 2, 3, 4, 4, 5, 2, 3, 4, 4, 5, 2, 3, 4, 4, 4, 6, 7, 7, 8, 9, 10, 11, 12, 13, 6, 6, 10, 9, 7, 7, 9, 14, 6, 12, 14, 12, 13, 15, 2, 3, 4, 4, 5, 2, 3, 4, 4, 5, 2, 3, 4, 4, 4, 6, 7, 9, 10, 9, 14, 12, 14, 5, 2, 3, 4, 4, 4, 6, 7, 7, 8, 16, 11, 12, 13, 6, 6, 10, 9, 7, 7, 9, 14, 6, 12, 14, 17, 9, 18, 9, 14, 16, 16, 8, 19, 10, 16, 8, 8, 20, 21, 20, 22, 23, 10, 24, 24, 9, 10, 24, 24, 9, 10, 24, 24, 25, 20, 25, 26, 23, 27, 28, 28, 28, 26, 27, 23, 29, 27, 23, 27, 30, 22, 23, 31, 8, 8, 23, 10, 16, 16, 10, 9, 7, 7, 9, 14, 32, 14, 12, 4, 7, 16, 33, 10, 9, 8, 8, 24, 18, 9, 14, 16, 16, 10, 9, 7, 7, 9, 14, 34, 17, 14, 12, 35, 0, 0, 0, 1, 2, 3, 4, 4, 5, 2, 3, 4, 4, 5, 2, 3, 4, 4, 4, 6, 7, 7, 8, 9, 10, 11, 12, 13, 6, 6, 10, 9, 7, 7, 9, 14, 6, 12, 14, 12, 13, 15, 2, 3, 4, 4, 5, 2, 3, 4, 4, 5, 2, 3, 4, 4, 4, 6, 7, 9, 10, 9, 14, 12, 14, 5, 2, 3, 4, 4, 4, 6, 7, 7, 8, 16, 11, 12, 13, 6, 6, 10, 9, 7, 7, 9, 14, 6, 12, 14, 17, 9, 18, 9, 14, 16, 16, 8, 19, 10, 16, 8, 8, 20, 21, 20, 22, 23, 10, 24, 24, 9, 10, 24, 24, 9, 10, 24, 24, 25, 20, 25, 26, 23, 27, 28, 28, 28, 26, 27, 23, 29, 27, 23, 27, 30, 22, 23, 31, 8, 8, 23, 10, 16, 16, 10, 9, 7, 7, 9, 14, 32, 14, 12, 4, 7, 16, 33, 10, 9, 8, 8, 24, 18, 9, 14, 16, 16, 10, 9, 7, 7, 9, 14, 34, 17, 14, 12, 35, 35, 35, 35, 35, 36, 35, 36], [37, 38, 39, 38, 24, 40, 38, 16, 8, 18, 6, 37, 38, 39, 38, 24, 41, 42, 4, 43, 44, 45, 46, 47, 48, 47, 8, 49, 50, 3, 4, 5, 51, 50, 52, 42, 12, 53, 50, 54, 42, 55, 47, 56, 57, 58, 59, 60, 58, 12, 53, 50, 54, 42, 55, 47, 56, 57, 58, 59, 60, 58, 12, 61, 0, 37, 38, 39, 38, 24, 40, 38, 16, 8, 18, 6, 37, 38, 39, 38, 24, 41, 42, 4, 43, 44, 45, 46, 47, 48, 47, 8, 49, 50, 3, 4, 5, 51, 50, 52, 42, 12, 62, 55, 47, 39, 38, 37, 63, 20, 63, 21, 57, 64, 9, 62, 55, 47, 39, 38, 37, 63, 20, 63, 21, 57, 64, 9, 61], [0, 0, 0, 1, 65, 66, 65, 67, 3, 6, 68, 65, 66, 65, 67, 3, 69, 68, 10, 10, 70, 70, 9, 9, 71, 71, 17, 17, 14, 14, 12, 12, 72, 72, 73, 73, 74, 74, 50, 54, 62, 55, 14, 75, 65, 66, 65, 67, 3, 76, 68, 65, 66, 65, 67, 3, 7, 68, 31, 31, 77, 77, 10, 10, 70, 70, 9, 9, 71, 71, 17, 17, 14, 14, 73, 73, 72, 72, 55, 47, 39, 38, 10, 78, 57, 79, 57, 59, 57, 59, 64, 59, 64, 80, 38, 47, 62, 81, 75, 59, 82, 59, 64, 59, 64, 58, 64, 58, 83, 37, 46, 39, 12, 75, 64, 59, 64, 58, 64, 58, 21, 58, 21, 63, 21, 58, 63, 21, 58, 63, 21, 58, 84, 63, 21, 84, 63, 21, 84, 63, 21, 29, 68, 65, 66, 65, 67, 3, 6, 68, 65, 66, 65, 67, 3, 69, 68, 10, 10, 70, 70, 9, 9, 71, 71, 17, 17, 14, 14, 12, 12, 72, 72, 73, 73, 74, 74, 50, 54, 62, 55, 14, 75, 65, 66, 65, 67, 3, 76, 68, 65, 66, 65, 67, 3, 7, 68, 31, 31, 77, 77, 10, 10, 70, 70, 9, 9, 71, 71, 17, 17, 14, 14, 73, 73, 72, 72, 55, 47, 39, 38, 10, 78, 57, 79, 57, 59, 57, 59, 64, 59, 64, 80, 38, 47, 62, 81, 75, 59, 82, 59, 64, 59, 64, 58, 64, 58, 83, 37, 46, 39, 12, 75, 64, 59, 64, 58, 64, 58, 21, 58, 21, 63, 21, 58, 63, 21, 58, 63, 21, 58, 84, 63, 21, 84, 63, 21, 84, 63, 21, 29, 68, 85, 37, 86, 37, 29, 87, 37, 8, 88, 89, 7, 85, 37, 86, 37, 29, 90, 55, 6, 16, 91, 76, 38, 39, 46, 39, 88, 52, 54, 45, 6, 8, 92, 62, 76, 8, 12, 50, 54, 62, 55, 47, 39, 58, 59, 21, 58, 59, 21, 71, 50, 54, 62, 55, 47, 39, 58, 59, 21, 58, 59, 21, 12, 75, 90, 55, 6, 68], [0, 62, 93, 62, 93, 62, 93, 62, 93, 42, 93, 62, 93, 94, 93, 62, 93, 62, 93, 62, 93, 62, 93, 42, 93, 62, 93, 94, 93, 62, 93, 62, 93, 13, 75, 13, 75, 42, 93, 42, 93, 95, 93, 42, 93, 96, 97, 62, 93, 62, 93, 62, 93, 62, 93, 42, 93, 62, 93, 94, 93, 62, 93, 62, 93, 62, 93, 62, 93, 42, 93, 62, 93, 94, 93, 62, 93, 62, 93, 42, 93, 42, 93, 4, 75, 42, 93, 95, 93, 42, 93, 98, 1], [69, 99, 8, 100, 101, 100, 37, 93, 86, 93, 102, 93, 103, 93, 104, 93, 88, 70, 47, 93, 105], [106, 101, 75, 107, 10, 71, 69, 99, 8, 100, 101, 100, 37, 93, 86, 93, 102, 93, 103], [93, 92, 93, 7, 72, 50, 93, 31, 27, 88, 75, 71, 12, 73, 45, 76, 6, 69, 88], [69, 55, 93, 62, 93, 52, 93, 49, 93, 92, 93, 7, 72, 50, 93, 31, 27, 88, 75, 71, 12, 73, 45], [99, 8, 100, 101, 100, 37, 93, 86, 93, 102, 93, 103, 93, 104, 93, 88, 70, 47, 93, 105], [106, 101, 75, 107, 10, 71, 69, 99, 8, 100, 101, 100, 37, 93, 86, 93, 102, 93, 103], [93, 92, 93, 7, 72, 50, 93, 31, 27, 88, 75, 71, 12, 73, 45, 76, 6, 69, 88], [69, 55, 93, 62, 93, 52, 93, 49, 93, 92, 93, 7, 72, 50, 93, 31, 27, 88, 75, 71, 12, 73, 45], [99, 8, 100, 101, 100, 37, 93, 86, 93, 102, 93, 103, 93, 104, 93, 88, 70, 47, 93, 105], [106, 101, 75, 107, 10, 71, 69, 99, 8, 100, 101, 100, 37, 93, 86, 93, 102, 93, 103], [93, 92, 93, 7, 72, 50, 93, 31, 27, 88, 75, 71, 12, 73, 45, 76, 6, 69, 88], [69, 55, 93, 62, 93, 52, 93, 49, 93, 92, 93, 7, 72, 50, 93, 31, 27, 88, 75, 71, 12, 73, 45], [99, 8, 100, 101, 100, 37, 93, 86, 93, 102, 93, 103, 93, 104, 93, 88, 70, 47, 93, 105], [77, 16, 75, 12, 74, 108, 4, 7, 109, 14, 93, 17, 16, 90, 93, 16, 55, 93, 62, 93, 4], [4, 104, 13, 93, 72, 94, 93, 110, 93, 111, 95, 93, 48, 93, 92, 93, 92, 93, 92, 93, 76, 41], [93, 112, 93, 112, 93, 88, 77, 75, 113, 93, 113, 93, 113, 93, 87, 93, 87, 93, 87, 93, 87, 93, 6], [99, 8, 100, 101, 100, 37, 93, 86, 93, 102, 93, 103, 93, 104, 93, 88, 70, 47, 93, 105], [106, 101, 75, 107, 10, 71, 69, 99, 8, 100, 101, 100, 37, 93, 86, 93, 102, 93, 103], [93, 92, 93, 7, 72, 50, 93, 31, 27, 88, 75, 71, 12, 73, 45, 76, 6, 69, 88], [69, 55, 93, 62, 93, 52, 93, 49, 93, 92, 93, 7, 72, 50, 93, 31, 27, 88, 75, 71, 12, 73, 45], [99, 8, 100, 101, 100, 37, 93, 86, 93, 102, 93, 103, 93, 104, 93, 88, 70, 47, 93, 105], [27, 88, 75, 71, 12, 73, 52, 114, 52, 114, 73, 75, 52, 114, 52, 114, 73, 75, 3], [75, 3, 75, 49, 93, 45, 75, 52, 114, 52, 93, 45, 75, 52, 114, 52, 93, 45], [75, 52, 114, 73, 75, 3, 75, 49, 114, 81, 75, 45, 75, 52, 114, 52, 93, 7], [45, 40, 73, 93, 13, 6, 49, 93, 6, 50, 93, 115, 93, 2, 45, 40, 73, 93, 13, 90], [93, 49, 93, 6, 50, 93, 55, 93, 41, 93, 41, 93, 41, 93, 4, 52, 93, 52, 93, 52, 93, 45, 74], [75, 113, 93, 113, 93, 113, 93, 87, 93, 87, 93, 87, 93, 87, 93, 116, 117, 118, 114, 93, 119, 93], [120, 121, 122, 123, 88, 124, 125, 126, 93, 30, 101, 127, 93, 101, 128, 93, 129, 93, 88, 124, 125, 126, 93, 30, 130, 93, 127, 93, 101, 128, 93, 131, 93, 132, 93, 132, 93, 132, 93, 28, 133, 93, 133, 93, 133, 93, 124, 27, 75, 134, 93, 134, 93, 134, 93, 127, 93, 127, 93, 127, 93, 127, 93, 88, 124, 125, 126, 93, 30, 101, 127, 93, 101, 128, 93, 129, 93, 88, 124, 125, 126, 93, 30, 130, 93, 127, 93, 101, 128, 93, 131, 93, 132, 93, 132, 93, 132, 93, 28, 133, 93, 133, 93, 133, 93, 124, 27, 75, 134, 93, 134, 93, 134, 93, 127, 93, 127, 93, 127, 93, 127, 93, 107, 75, 127, 93, 127, 93, 127, 93, 128, 93, 128, 93, 128, 93, 128, 93, 128, 93, 128, 93, 128, 93, 128, 93, 128, 93, 128, 93, 128, 93, 128, 93, 128, 93, 128, 93, 128, 93, 128, 93, 69, 99, 8, 100, 101, 100, 37, 93, 86, 93, 102, 93, 103, 93, 104, 93, 88, 70, 47, 93, 105, 106, 101, 75, 107, 10, 71, 69, 99, 8, 100, 101, 100, 37, 93, 86, 93, 102, 93, 103, 93, 104, 93, 88, 70, 47, 93, 105, 106, 101, 75, 107, 10, 71, 69, 99, 8, 100, 101, 100, 37, 93, 86, 93, 102, 93, 103, 93, 104, 93, 88, 70, 47, 93, 105, 106, 101, 75, 107, 10, 71, 69, 99, 8, 100, 101, 100, 37, 93, 86, 93, 102, 93, 103, 93, 104, 93, 88, 70, 47, 93, 39, 93, 39, 93, 39, 93, 71, 1], [17, 135, 68, 17, 77, 10, 77, 23, 77, 23, 136, 68, 23, 77, 23, 27, 9, 10, 77, 137, 138, 12, 139, 17, 9, 13, 17, 139, 12, 77, 10, 9, 23, 77, 10, 77, 23, 9, 10, 17, 14, 13, 9, 10, 77, 10, 9, 13, 9, 10, 77, 10, 9, 10, 17, 43, 14, 140, 17, 135, 68, 17, 77, 10, 77, 23, 77, 23, 136, 68, 23, 77, 23, 27, 9, 10, 77, 137, 138, 12, 139, 17, 9, 13, 17, 139, 12, 77, 10, 9, 23, 77, 10, 77, 23, 9, 10, 17, 14, 13, 9, 10, 77, 10, 9, 13, 9, 10, 77, 10, 9, 10, 17, 43, 14, 140, 23, 19, 68, 27, 77, 23, 27, 24, 77, 19, 8, 27, 9, 10, 77, 8, 9, 71, 12, 71, 9, 12, 9, 10, 12, 10, 77, 23, 27, 23, 27, 77, 8, 9, 141, 68, 77, 113, 129, 10, 87, 142, 9, 40, 37, 17, 143, 38, 139, 94, 46, 12, 12, 139, 17, 9, 10, 77, 10, 17, 10, 77, 23, 27, 23, 27, 126, 27, 23, 77, 19, 43, 77, 16, 23, 8, 27, 77, 126, 27, 23, 27, 77, 10, 77, 23, 17, 77, 10, 9, 10, 77, 10, 17, 77, 13, 77, 10, 144, 23, 19, 68, 27, 77, 23, 27, 24, 77, 19, 8, 27, 9, 10, 77, 8, 9, 71, 12, 71, 9, 12, 9, 10, 12, 10, 77, 23, 27, 23, 27, 77, 8, 9, 141, 68, 77, 113, 129, 10, 87, 142, 9, 40, 37, 17, 143, 38, 139, 94, 46, 12, 12, 139, 17, 9, 10, 77, 10, 17, 10, 77, 23, 27, 23, 27, 126, 27, 23, 77, 19, 43, 77, 16, 23, 8, 27, 77, 126, 27, 23, 27, 77, 10, 77, 23, 17, 77, 10, 9, 10, 77, 10, 17, 77, 13, 77, 10, 144], [17, 135, 145, 137, 141, 135, 137, 146, 147, 24, 77, 8, 17, 16, 17, 7, 13, 43, 17, 138, 31, 19, 145, 148, 31, 24, 13, 16, 31, 24, 13, 16, 14, 43, 9, 149, 4, 17, 135, 145, 137, 16, 9, 135, 137, 146, 88, 31, 24, 77, 8, 17, 16, 17, 7, 13, 43, 17, 138, 31, 19, 150, 151, 148, 31, 24, 13, 16, 31, 24, 13, 16, 14, 43, 9, 149, 4, 23, 19, 152, 136, 68, 27, 30, 27, 30, 107, 31, 10, 77, 153, 106, 154, 151, 154, 155, 78, 107, 10, 107, 27, 30, 151, 28, 27, 156, 10, 27, 23, 77, 10, 77, 10, 16, 23, 43, 77, 7, 10, 6, 9, 4, 17, 17, 9, 10, 138, 9, 24, 77, 10, 9, 17, 157, 135, 23, 77, 23, 10, 9, 17, 77, 10, 9, 23, 77, 10, 27, 30, 151, 30, 151, 77, 24, 77, 137, 146, 24, 10, 138, 23, 140, 23, 19, 152, 136, 68, 27, 30, 27, 30, 107, 31, 10, 77, 153, 106, 154, 151, 154, 155, 78, 107, 10, 107, 27, 30, 151, 28, 27, 156, 10, 27, 23, 77, 10, 77, 10, 16, 23, 43, 77, 7, 10, 6, 9, 4, 17, 17, 9, 10, 138, 9, 24, 77, 10, 9, 17, 157, 135, 23, 77, 23, 10, 9, 17, 77, 10, 9, 23, 77, 10, 27, 30, 151, 30, 151, 77, 24, 77, 137, 146, 24, 10, 138, 23, 140], [158, 13, 73, 13, 159, 93, 7, 6, 6, 4, 160, 93, 13, 161, 114, 81, 73, 12, 72, 73, 76, 162, 93, 73, 4, 45, 163, 93, 81, 164, 12, 10, 9, 69, 16, 165, 93, 12, 166, 73, 13, 14, 16, 69, 167, 93, 71, 158, 13, 73, 81, 168, 93, 14, 12, 13, 73, 166, 73, 13, 14, 169, 93, 43, 111, 159, 93, 170, 93, 14, 171, 97, 158, 13, 73, 13, 159, 93, 7, 6, 6, 4, 160, 93, 13, 161, 114, 81, 73, 12, 72, 73, 76, 162, 93, 73, 4, 45, 163, 93, 81, 164, 12, 10, 9, 69, 16, 165, 93, 12, 166, 73, 13, 14, 16, 69, 167, 93, 71, 158, 13, 73, 81, 168, 93, 14, 12, 13, 73, 166, 73, 13, 14, 169, 93, 43, 111, 159, 93, 170, 93, 14, 171, 97, 158, 13, 73, 13, 159, 93, 7, 6, 6, 4, 160, 93, 13, 161, 114, 81, 73, 12], [172, 173, 101, 174, 175, 111, 150, 176, 105, 177, 161, 93, 178, 31, 72, 70, 77, 70, 69, 77, 75, 111, 179, 93, 151, 76, 139, 77, 70, 9, 70, 107, 31, 126, 180, 75, 181, 70, 30, 182, 93, 99, 138, 183, 75, 184, 93, 185, 31, 75, 186, 93, 182, 93, 187, 75, 172, 173, 101, 174, 175, 111, 150, 176, 105, 177, 161, 93, 178, 31, 72, 70, 77, 70, 69, 77, 75, 111, 179, 93, 151, 76, 139, 77, 70, 9, 70, 107, 31, 126, 180, 75, 181, 70, 30, 182, 93, 99, 138, 183, 75, 184, 93, 185, 31, 75, 186, 93, 182, 93, 187, 75, 172, 173, 101, 174, 175, 111, 150, 176, 105], [158, 13, 73, 13, 159, 93, 7, 6, 6, 4, 160, 93, 13, 161, 114, 81, 73, 12, 72, 73, 76, 162, 93, 73, 4, 45, 163, 93, 81, 164, 12, 10, 9, 69, 16, 165, 93, 12, 166, 73, 13, 14, 16, 69, 167, 93, 71, 158, 13, 73, 81, 168, 93, 14, 12, 13, 73, 166, 73, 13, 14, 169, 93, 43, 111, 159, 93, 170, 93, 14, 188, 75, 158, 13, 73, 13, 159, 93, 7, 6, 6, 4, 160, 93, 13, 161, 114, 81, 73, 12, 72, 73, 76, 162, 93, 73, 4, 45, 163, 93, 81, 164, 12, 10, 9, 69, 16, 165, 93, 12, 166, 73, 13, 14, 16, 69, 167, 93, 71, 158, 13, 73, 81, 168, 93, 14, 12, 13, 73, 166, 73, 13, 14, 169, 93, 43, 111, 159, 93, 170, 93, 14, 188, 75, 158, 13, 73, 13, 159, 93, 7, 6, 6, 4, 160, 93, 13, 161, 114, 81, 73, 12], [170, 93, 72, 96, 93, 165, 93, 73, 161, 93, 189, 93, 71, 190, 93, 179, 93, 77, 138, 99, 191, 93, 139, 182, 93, 0, 111, 170, 93, 72, 96, 93, 179, 93, 77, 192, 93, 191, 93, 139, 182, 93, 193, 93, 70, 99, 69, 194, 93, 17, 195, 93, 179, 93, 77, 138, 43, 191, 93, 139, 182, 93, 196, 75, 186, 93, 182, 93, 99, 69, 43, 111, 170, 93, 72, 96, 93, 165, 93, 73, 161, 93, 189, 93, 71, 190, 93, 179, 93, 77, 138, 99, 191, 93, 139, 182, 93, 0, 111, 170, 93, 72, 96, 93, 179, 93, 77, 192, 93, 191, 93, 139, 182, 93, 193, 93, 70, 99, 69, 194, 93, 17, 195, 93, 179, 93, 77, 138, 43, 191, 93, 139, 182, 93, 196, 75, 186, 93, 182, 93, 99, 69, 43, 111, 170, 93, 72, 96, 93, 165, 93, 73, 161, 93, 189, 93, 71, 190, 93, 179, 93, 77, 138, 99], [12, 10, 197, 8, 12, 10, 99, 198, 199, 156, 12, 10, 200, 10, 107, 10, 27, 151, 201, 132, 93, 151, 30, 151, 27, 28, 28, 126, 27, 126, 23, 29, 29, 27, 107, 27, 77, 10, 9, 17, 14, 32, 6, 27, 106, 202, 156, 27, 106, 185, 203, 204, 205, 27, 106, 206, 106, 207, 106, 208, 209, 210, 211, 93, 209, 212, 209, 208, 213, 213, 214, 208, 214, 215, 216, 216, 208, 207, 208, 153, 106, 154, 151, 30, 22, 134, 93, 12, 10, 197, 8, 12, 10, 99, 198, 199, 156, 12, 10, 200, 10, 107, 10, 27, 151, 201, 132, 93, 151, 30, 151, 27, 28, 28, 126, 27, 126, 23, 29, 29, 27, 107, 27, 77, 10, 9, 17, 14, 32, 90, 93, 27, 106, 202, 156, 27, 106, 185, 203, 204, 205, 27, 106, 206, 106, 207, 106, 208, 209, 210, 211, 93, 209, 212, 209, 208, 213, 213, 214, 208, 214, 215, 216, 216, 208, 207, 208, 153, 106, 154, 151, 30, 22, 134, 93, 12, 10, 197, 8, 12, 10, 99, 198, 199, 156, 12, 10, 200, 10, 107, 10, 27, 151, 201, 132, 93, 151, 30, 151, 27, 28, 28, 126, 27, 126, 23, 29, 29, 27, 107, 27, 77, 10, 9, 17, 14, 32, 90, 93, 27, 106, 202, 156, 27, 106, 185, 203, 204, 205, 27, 106, 206, 106, 207, 106, 208, 209, 210, 211, 93, 209, 212, 209, 208, 213, 213, 214, 208, 214, 215, 216, 216, 208, 207, 208, 153, 106, 154, 151, 30, 22, 29, 29, 0], [68, 217, 100, 100, 68, 218, 219, 219, 24, 100, 23, 10, 150, 28, 220, 156, 221, 93, 222, 223, 93, 224, 133, 93, 152, 10, 77, 17, 43, 111, 4, 76, 92, 225], [68, 217, 100, 100, 68, 218, 219, 219, 24, 100, 23, 10, 150, 28, 220, 156, 221, 93, 222, 223, 93, 224, 133, 93, 152, 10, 77, 17, 43, 111, 4, 76, 92, 225], [68, 217, 100, 100, 68, 218, 219, 219, 24, 100, 23, 10, 150, 28, 220, 156, 221, 93, 222, 223, 93, 224, 133, 93, 152, 10, 77, 17, 43, 111, 4, 76, 92, 225], [68, 31, 10, 106, 30, 27, 173, 23, 106, 31, 10, 106, 30, 27, 173, 23, 106, 31, 10, 106, 30, 27, 173, 23, 106, 208, 209, 226, 208, 106, 105, 106, 173, 215, 226, 106, 30, 215, 30, 14, 30, 226, 106, 154, 27, 226, 27, 12, 27, 226, 106, 226, 151, 10, 151, 106, 208, 208, 106, 106, 27, 106, 30, 30, 10, 0], [68, 31, 10, 106, 30, 27, 173, 23, 106, 31, 10, 106, 30, 27, 173, 23, 106, 31, 10, 106, 30, 27, 173, 23, 106, 208, 209, 226, 208, 106, 105, 106, 173, 215, 226, 106, 30, 215, 30, 14, 30, 226, 106, 154, 27, 226, 27, 12, 27, 226, 106, 226, 151, 10, 151, 106, 208, 208, 106, 106, 27, 106, 30, 30, 10, 0], [68, 31, 10, 106, 30, 27, 173, 23, 106, 31, 10, 106, 30, 27, 173, 23, 106, 31, 10, 106, 30, 27, 173, 23, 106, 208, 209, 226, 208, 106, 105, 106, 173, 215, 226, 106, 30, 215, 30, 14, 30, 226, 106, 154, 27, 226, 27, 12, 27, 226, 106, 226, 151, 10, 151, 106, 208, 208, 106, 106, 27, 106, 30, 30, 10, 0], [12, 10, 197, 8, 12, 10, 99, 198, 199, 156, 12, 10, 200, 10, 107, 10, 27, 151, 201, 132, 93, 151, 30, 151, 27, 28, 28, 126, 27, 126, 23, 29, 29, 27, 107, 27, 77, 10, 9, 17, 14, 32, 6, 27, 106, 202, 156, 27, 106, 185, 203, 204, 205, 27, 106, 206, 106, 207, 106, 208, 209, 210, 211, 93, 209, 212, 209, 208, 213, 213, 214, 208, 214, 215, 216, 216, 208, 207, 208, 153, 106, 154, 151, 30, 22, 134, 93, 12, 10, 197, 8, 12, 10, 99, 198, 199, 156, 12, 10, 200, 10, 107, 10, 27, 151, 201, 132, 93, 151, 30, 151, 27, 28, 28, 126, 27, 126, 23, 29, 29, 27, 107, 27, 77, 10, 9, 17, 14, 32, 90, 93, 27, 106, 202, 156, 27, 106, 185, 203, 204, 205, 27, 106, 206, 106, 207, 106, 208, 209, 210, 211, 93, 209, 212, 209, 208, 213, 213, 214, 208, 214, 215, 216, 216, 208, 207, 208, 153, 106, 154, 151, 30, 22, 134, 93, 12, 10, 197, 8, 12, 10, 99, 198, 199, 156, 12, 10, 200, 10, 107, 10, 27, 151, 201, 132, 93, 151, 30, 151, 27, 28, 28, 126, 27, 126, 23, 29, 29, 27, 107, 27, 77, 10, 9, 17, 14, 32, 90, 93, 27, 106, 202, 156, 27, 106, 185, 203, 204, 205, 27, 106, 206, 106, 207, 106, 208, 209, 210, 211, 93, 209, 212, 209, 208, 213, 213, 214, 208, 214, 215, 216, 216, 208, 207, 208, 153, 106, 154, 151, 30, 22, 29, 29, 0], [68, 217, 100, 100, 68, 218, 219, 219, 24, 100, 23, 10, 150, 28, 220, 156, 221, 93, 222, 223, 93, 224, 133, 93, 152, 10, 77, 17, 43, 111, 4, 76, 92, 225], [68, 217, 100, 100, 68, 218, 219, 219, 24, 100, 23, 10, 150, 28, 220, 156, 221, 93, 222, 223, 93, 224, 133, 93, 152, 10, 77, 17, 43, 111, 4, 76, 92, 225], [68, 217, 100, 100, 68, 218, 219, 219, 24, 100, 23, 10, 150, 28, 220, 156, 221, 93, 222, 223, 93, 224, 133, 93, 152, 10, 77, 17, 43, 111, 4, 76, 92, 225], [68, 31, 10, 106, 30, 27, 173, 23, 106, 31, 10, 106, 30, 27, 173, 23, 106, 31, 10, 106, 30, 27, 173, 23, 106, 208, 209, 226, 208, 106, 105, 106, 173, 215, 226, 106, 30, 215, 30, 14, 30, 226, 106, 154, 27, 226, 27, 12, 27, 226, 106, 226, 151, 10, 151, 106, 208, 208, 106, 106, 27, 106, 30, 30, 10, 0], [68, 31, 10, 106, 30, 27, 173, 23, 106, 31, 10, 106, 30, 27, 173, 23, 106, 31, 10, 106, 30, 27, 173, 23, 106, 208, 209, 226, 208, 106, 105, 106, 173, 215, 226, 106, 30, 215, 30, 14, 30, 226, 106, 154, 27, 226, 27, 12, 27, 226, 106, 226, 151, 10, 151, 106, 208, 208, 106, 106, 27, 106, 30, 30, 10, 0], [68, 31, 10, 106, 30, 27, 173, 23, 106, 31, 10, 106, 30, 27, 173, 23, 106, 31, 10, 106, 30, 27, 173, 23, 106, 208, 209, 226, 208, 106, 105, 106, 173, 215, 226, 106, 30, 215, 30, 14, 30, 226, 106, 154, 27, 226, 27, 12, 27, 226, 106, 226, 151, 10, 151, 106, 208, 208, 106, 106, 27, 106, 30, 30, 10, 0], [68, 143, 93, 109, 93, 143, 93, 109, 93, 143, 93, 109, 93, 143, 93, 109, 93, 143, 93, 109, 93, 143, 93, 109, 93, 40, 93, 134, 93, 41, 93, 109, 93, 143, 93, 109, 93, 103, 93, 109, 93, 94, 93, 113, 93, 90, 93, 113, 93, 92, 93, 113, 93, 70, 14, 17, 13, 127, 93, 113, 93, 143, 93], [68, 143, 93, 109, 93, 143, 93, 109, 93, 143, 93, 109, 93, 143, 93, 109, 93, 143, 93, 109, 93, 143, 93, 109, 93, 40, 93, 134, 93, 41, 93, 109, 93, 143, 93, 109, 93, 103, 93, 109, 93, 94, 93, 113, 93, 90, 93, 113, 93, 92, 93, 113, 93, 70, 14, 17, 13, 127, 93, 113, 93, 143, 93], [68, 143, 93, 109, 93, 143, 93, 109, 93, 143, 93, 109, 93, 143, 93, 109, 93, 143, 93, 109, 93, 143, 93, 109, 93, 40, 93, 134, 93, 41, 93, 109, 93, 143, 93, 109, 93, 103, 93, 109, 93, 94, 93, 113, 93, 90, 93, 113, 93, 92, 93, 113, 93, 70, 14, 17, 13, 127, 93, 113, 93, 143, 93]]\n"
     ]
    }
   ],
   "source": [
    "print(data_encoded)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number of notes per sequence to train the model on\n",
    "sequence_length = 10\n",
    "\n",
    "X = []\n",
    "Y = []\n",
    "\n",
    "# given data_encoded, generate training data by looping\n",
    "for i in range(len(data_encoded)):\n",
    "    \n",
    "    for j in range(len(data_encoded[i]) - sequence_length):\n",
    "        # from j to j + sequence_length, append to X\n",
    "        X.append(data_encoded[i][j:j + sequence_length])\n",
    "        # append the next note to Y for prediction purposes\n",
    "        Y.append(data_encoded[i][j + sequence_length])\n",
    "\n",
    "\n",
    "\n",
    "# one-hot encode cache\n",
    "encodings_onehot = {}\n",
    "\n",
    "# creating an array of values where 1 is mapped to the corresponding dictionary value in encodings\n",
    "# k is note tuple, v is encoding index\n",
    "for k, v in encodings.items():\n",
    "    #create a new array of zeros with length of encodings\n",
    "    onehot = np.zeros(len(encodings))\n",
    "    #set the index of the note to 1\n",
    "    onehot[v] = 1\n",
    "    #append to encodings_onehot\n",
    "    encodings_onehot[v] = onehot\n",
    "\n",
    "# one-hot encode X\n",
    "# for each sequence in X, we will convert it to a one-hot encoding\n",
    "X_onehot = []\n",
    "for seq in X:\n",
    "    X_onehot.append(np.array([ encodings_onehot[note] for note in seq]))\n",
    "X = X_onehot\n",
    "\n",
    "# split this into training and testing sets \n",
    "# we will intentionally overfit, GET RID OF THIS LATER\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/5\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 6ms/step - accuracy: 0.1149 - loss: 4.9517\n",
      "Epoch 2/5\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.3332 - loss: 3.1212\n",
      "Epoch 3/5\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.7265 - loss: 1.5833\n",
      "Epoch 4/5\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.8787 - loss: 0.7176\n",
      "Epoch 5/5\n",
      "\u001b[1m93/93\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 6ms/step - accuracy: 0.9383 - loss: 0.3657\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1bc90da65c0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TODO: make the model itself\n",
    "# benchmark: dense is 80% accuracy\n",
    "\n",
    "\n",
    "model = tf.keras.models.Sequential([\n",
    "    tf.keras.Input(shape=(sequence_length, len(encodings))),\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(256, activation='relu'),\n",
    "    # tf.keras.layers.Dropout(.4),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dense(len(encodings), activation='softmax')\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "model.fit(np.array(X_train), np.array(Y_train), epochs=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m24/24\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 3ms/step - accuracy: 0.8321 - loss: 1.0142\n",
      "Loss: 1.0959199666976929, Accuracy: 0.8243243098258972\n"
     ]
    }
   ],
   "source": [
    "loss, accuracy = model.evaluate(np.array(X_test), np.array(Y_test), verbose=1)\n",
    "print(f'Loss: {loss}, Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 99ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
      "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
      "F 1.00\n",
      "A 1.00\n",
      "F 0.75\n",
      "A 0.50\n",
      "Rest 0.25\n",
      "A# 0.50\n",
      "C 1.00\n",
      "G 0.75\n",
      "Rest 0.25\n",
      "C 1.00\n",
      "G 0.25\n",
      "Rest 0.25\n",
      "F# 0.25\n",
      "Rest 0.25\n",
      "F 1.00\n",
      "B 0.25\n",
      "C 0.25\n",
      "B 0.25\n",
      "Rest 0.25\n",
      "B 0.25\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "# given a sequence, generate the next note (up to ten times)\n",
    "seq = copy.deepcopy(X_train[1])\n",
    "total_seq = seq[:]\n",
    "for i in range(10):\n",
    "\n",
    "    #2d array that predicts the probability of the next note\n",
    "    pred = model.predict(np.array([seq]))\n",
    "\n",
    "    #this line takes the prediction with the highest probability\n",
    "    pred = np.argmax(pred)\n",
    "\n",
    "    # append to sequence the one-hot encoding of pred\n",
    "    one_hot = np.zeros((len(encodings)))\n",
    "    one_hot[pred] = 1\n",
    "    total_seq = np.append(total_seq, [one_hot], axis=0)\n",
    "    # set sequence to be the last ten values of total_seq\n",
    "    seq = total_seq[-10:]\n",
    "\n",
    "    #total_seq.append(pred)\n",
    "    # total seq is a numpy array, can't just append things to it\n",
    "\n",
    "mapping = {0: \"C\", 1: \"C#\", 2: \"D\", 3: \"D#\", 4: \"E\", 5: \"F\", 6: \"F#\", 7: \"G\", 8: \"G#\", 9: \"A\", 10: \"A#\", 11: \"B\"}\n",
    "    \n",
    "for note in total_seq:\n",
    "    pitch, dur = decodings[np.argmax(note)]\n",
    "    print (mapping[(pitch - 1) % 12] if pitch != 0 else \"Rest\", dur)\n",
    "\n",
    "# convert numbers to musical note letters\n",
    "# 60 = C"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
